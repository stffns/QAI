"""
Test runner principal para todos los tests de configuraci√≥n

Este m√≥dulo permite ejecutar todos los tests de config de forma organizada
y generar reportes comprehensivos.

Uso:
    python tests/config/test_runner.py
    
    # O usar pytest directamente:
    pytest tests/config/ -v
    pytest tests/config/test_core_models.py -v
    pytest tests/config/test_settings.py -v  
    pytest tests/config/test_logging_models.py -v
"""

import sys
import os
from pathlib import Path

# Agregar el directorio ra√≠z al path para imports
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

def run_config_tests():
    """
    Ejecuta todos los tests de configuraci√≥n y genera reporte
    """
    import pytest
    
    # Configuraci√≥n de pytest
    args = [
        "tests/config/",
        "-v",                    # Verbose output
        "--tb=short",           # Traceback corto
        "--disable-warnings",   # Desabilitar warnings para output limpio
        "--color=yes",          # Output con colores
        "-x",                   # Parar en primer error
        "--durations=10",       # Mostrar 10 tests m√°s lentos
        "--cov=config",         # Coverage del m√≥dulo config
        "--cov-report=term-missing",  # Reporte de coverage
        "--cov-report=html:tests/config/htmlcov",  # Reporte HTML
    ]
    
    print("üß™ Ejecutando tests de configuraci√≥n...")
    print("=" * 60)
    
    # Ejecutar tests
    exit_code = pytest.main(args)
    
    print("\n" + "=" * 60)
    if exit_code == 0:
        print("‚úÖ Todos los tests de configuraci√≥n pasaron!")
        print("üìä Reporte de coverage generado en: tests/config/htmlcov/")
    else:
        print("‚ùå Algunos tests fallaron")
        print(f"üí• C√≥digo de salida: {exit_code}")
    
    return exit_code


def run_individual_test_files():
    """
    Ejecuta cada archivo de test individualmente para diagn√≥stico detallado
    """
    import pytest
    
    test_files = [
        ("Core Models", "tests/config/test_core_models.py"),
        ("Settings", "tests/config/test_settings.py"),
        ("Logging Models", "tests/config/test_logging_models.py")
    ]
    
    results = {}
    
    for name, test_file in test_files:
        print(f"\nüîç Ejecutando tests de {name}...")
        print("-" * 40)
        
        args = [
            test_file,
            "-v",
            "--tb=short",
            "--disable-warnings",
            "--color=yes"
        ]
        
        exit_code = pytest.main(args)
        results[name] = exit_code
        
        if exit_code == 0:
            print(f"‚úÖ {name}: PASSED")
        else:
            print(f"‚ùå {name}: FAILED (exit code: {exit_code})")
    
    print("\n" + "=" * 60)
    print("üìã RESUMEN DE RESULTADOS:")
    print("=" * 60)
    
    all_passed = True
    for name, exit_code in results.items():
        status = "‚úÖ PASSED" if exit_code == 0 else "‚ùå FAILED"
        print(f"{name:20} : {status}")
        if exit_code != 0:
            all_passed = False
    
    if all_passed:
        print("\nüéâ ¬°Todos los tests de configuraci√≥n pasaron!")
    else:
        print("\n‚ö†Ô∏è  Algunos tests fallaron. Revisar output arriba.")
    
    return 0 if all_passed else 1


def validate_test_environment():
    """
    Valida que el entorno est√© configurado correctamente para los tests
    """
    print("üîß Validando entorno de tests...")
    
    # Verificar imports principales
    try:
        import config
        print("‚úÖ M√≥dulo config importado correctamente")
    except ImportError as e:
        print(f"‚ùå Error importando config: {e}")
        return False
    
    try:
        from config.models.core import ModelConfig, DatabaseConfig, ToolsConfig
        print("‚úÖ Modelos core importados correctamente")
    except ImportError as e:
        print(f"‚ùå Error importando modelos core: {e}")
        return False
    
    try:
        from config.settings import Settings
        print("‚úÖ Settings importado correctamente")
    except ImportError as e:
        print(f"‚ùå Error importando Settings: {e}")
        return False
    
    try:
        from config.models.logging import LoggingConfig, LogLevel
        print("‚úÖ Modelos de logging importados correctamente")
    except ImportError as e:
        print(f"‚ùå Error importando modelos de logging: {e}")
        return False
    
    # Verificar pytest disponible
    try:
        import pytest
        print(f"‚úÖ pytest {pytest.__version__} disponible")
    except ImportError:
        print("‚ùå pytest no est√° instalado")
        return False
    
    print("‚úÖ Entorno de tests validado correctamente")
    return True


def main():
    """
    Funci√≥n principal del test runner
    """
    print("üöÄ QA Intelligence - Test Runner de Configuraci√≥n")
    print("=" * 60)
    
    # Validar entorno
    if not validate_test_environment():
        print("\n‚ùå Validaci√≥n de entorno fall√≥. Saliendo...")
        return 1
    
    # Preguntar modo de ejecuci√≥n
    print("\nüéØ Selecciona modo de ejecuci√≥n:")
    print("1. Ejecutar todos los tests juntos (r√°pido)")
    print("2. Ejecutar tests individuales (detallado)")
    print("3. Solo validar imports (diagn√≥stico)")
    
    choice = input("\nSelecci√≥n [1-3]: ").strip()
    
    if choice == "1":
        return run_config_tests()
    elif choice == "2":
        return run_individual_test_files()
    elif choice == "3":
        print("\n‚úÖ Validaci√≥n completada. Todos los imports funcionan.")
        return 0
    else:
        print("‚ùå Selecci√≥n inv√°lida")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
